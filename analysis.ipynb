{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d84bdc-8072-4fc1-8033-dc9ef2276951",
   "metadata": {},
   "source": [
    "# Week 1 Group Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70906237-8768-40f0-8d7f-653d106b2462",
   "metadata": {},
   "source": [
    "This notebook is a template workspace for the week 1 group projects involving\n",
    "the [Reproducible Brain Charts](https://reprobrainchart.github.io/) (RBC)\n",
    "database. Herein, we demonstrate how to access the RBC data and demonstrate a\n",
    "linear regression analysis on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953b424-fa57-4c10-b000-347a42254629",
   "metadata": {},
   "source": [
    "## Getting Started with RBC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5fa5-4ed4-44f8-b07c-e6252d450cf1",
   "metadata": {},
   "source": [
    "To load in some of the RBC data, we'll use some tools already installed on the\n",
    "HUB: `rbclib` and `pandas`.\n",
    "The `rbclib` library allows us to access RBC data from the cloud; it is\n",
    "demonstrated below.\n",
    "\n",
    "The `pandas` library handles spreadsheet data (called `DataFrame`s in Python)\n",
    "and can read tab-separated and comma-separated value files (`*.tsv` and\n",
    "`*.csv` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9811e8-12e7-4133-89e5-bce8eef6b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need the RBCPath type from the rbclib package to load data from the RBC.\n",
    "from rbclib import RBCPath\n",
    "\n",
    "# We'll also want to load some data directly from the filesystem.\n",
    "from pathlib import Path\n",
    "\n",
    "# We'll want to load/process some of the data using pandas and numpy.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e96e6-8fa1-4b3d-a952-bc2bb3ef26e7",
   "metadata": {},
   "source": [
    "### Accessing the PNC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bed2a-3869-47c3-9491-cfa57acb0688",
   "metadata": {},
   "source": [
    "The RBC project contains many datasets; for this project, we will focus on\n",
    "just one of these datasets: the\n",
    "[Philadelphia Neurodevelopmental Cohort](\n",
    "https://www.med.upenn.edu/bbl/philadelphianeurodevelopmentalcohort.html)\n",
    "(PNC). The PNC contains a lot of data, including raw MRI data. However, due to\n",
    "the time constraints for this project, we suggest that teams focus on the\n",
    "already processed data provided by the RBC, which is described below.\n",
    "\n",
    "The RBC's data is stored in a combination of GitHub repositories and Amazon S3\n",
    "buckets. The RBC GitHub repositories all belong to the organization\n",
    "[`ReproBrainChart`](https://github.com/ReproBrainChart), and each contains a\n",
    "subset of the data for one of the RBC datasets; for the PNC dataset, all\n",
    "repositories names start with `PNC_`:\n",
    "\n",
    "* `PNC_FreeSurfer`: structural data processed by FreeSurfer.\n",
    "* `PNC_BIDS`: raw MRI scan data in the\n",
    "  [Brain Imaging Data Structure](https://bids.neuroimaging.io/index.html)\n",
    "  format.\n",
    "* `PNC_CPAC`: processed functional MRI data.\n",
    "\n",
    "One typically accesses the RBC using the [`datalad`](https://www.datalad.org/)\n",
    "tool (see the [RBC page on accessing the data](\n",
    "https://reprobrainchart.github.io/docs/get_data) for more information).\n",
    "However, we will access the data using the `RBCPath` type that was imported in\n",
    "the code-cell above (`from rbclib import RBCPath`). This type inherits from a\n",
    "type called `CloudPath` (from the library [`cloudpathlib`](\n",
    "https://cloudpathlib.drivendata.org/stable/)); it represents the path of a\n",
    "file in the RBC dataset and can be used to access data in thecloud as if it\n",
    "were local.\n",
    "\n",
    "For example, the following cell creates an `RBCPath` to a subject's FreeSurfer\n",
    "data directory then lists and prints the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3489ab5-effd-408e-a633-88ddd68be768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_brainmeasures.json')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_brainmeasures.tsv')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_freesurfer.tar.xz')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_fsLR_den-164k.tar.xz')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_fsaverage.tar.xz')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_regionsurfacestats.tsv')\n"
     ]
    }
   ],
   "source": [
    "# This path refers to the repo github.com:ReproBrainChart/PNC_FreeSurfer;\n",
    "# Subject 1000393599's directory is used as an example.\n",
    "subject_id = 1000393599\n",
    "# To browse the repo, use this link:\n",
    "# https://github.com/ReproBrainChart/PNC_FreeSurfer/tree/main\n",
    "sub_path = RBCPath(f'rbc://PNC_FreeSurfer/freesurfer/sub-{subject_id}')\n",
    "\n",
    "# This path refers to a directory:\n",
    "assert sub_path.is_dir()\n",
    "\n",
    "# Print each file in the directory:\n",
    "for file in sub_path.iterdir():\n",
    "    print(repr(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726ff69-66cc-4ec7-84f4-04421055d7a6",
   "metadata": {},
   "source": [
    "If we want to open and load one of these files, we can do so using the\n",
    "`RBCPath.open` method. This method is like the `Path.open` method (from the\n",
    "built-in Python library [`pathlib`](1)). For example, if we want to load this\n",
    "subject's `regionsurfacestats.tsv` file, we can do so as follows.\n",
    "\n",
    "[1]: https://docs.python.org/3/library/pathlib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a190573-7d9d-4260-9986-0290e9b5adf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rbc://PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_regionsurfacestats.tsv ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>atlas</th>\n",
       "      <th>hemisphere</th>\n",
       "      <th>StructName</th>\n",
       "      <th>NumVert</th>\n",
       "      <th>SurfArea</th>\n",
       "      <th>GrayVol</th>\n",
       "      <th>ThickAvg</th>\n",
       "      <th>ThickStd</th>\n",
       "      <th>...</th>\n",
       "      <th>StdDev_wgpct</th>\n",
       "      <th>Min_wgpct</th>\n",
       "      <th>Max_wgpct</th>\n",
       "      <th>Range_wgpct</th>\n",
       "      <th>SNR_wgpct</th>\n",
       "      <th>Mean_piallgi</th>\n",
       "      <th>StdDev_piallgi</th>\n",
       "      <th>Min_piallgi</th>\n",
       "      <th>Max_piallgi</th>\n",
       "      <th>Range_piallgi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aparc.DKTatlas</td>\n",
       "      <td>lh</td>\n",
       "      <td>caudalanteriorcingulate</td>\n",
       "      <td>1668</td>\n",
       "      <td>1121</td>\n",
       "      <td>3493</td>\n",
       "      <td>2.870</td>\n",
       "      <td>0.588</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8371</td>\n",
       "      <td>-1.8413</td>\n",
       "      <td>42.8855</td>\n",
       "      <td>44.7269</td>\n",
       "      <td>4.4281</td>\n",
       "      <td>1.9877</td>\n",
       "      <td>0.0777</td>\n",
       "      <td>1.8054</td>\n",
       "      <td>2.1455</td>\n",
       "      <td>0.3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aparc.DKTatlas</td>\n",
       "      <td>lh</td>\n",
       "      <td>caudalmiddlefrontal</td>\n",
       "      <td>3308</td>\n",
       "      <td>2236</td>\n",
       "      <td>7030</td>\n",
       "      <td>2.882</td>\n",
       "      <td>0.537</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6666</td>\n",
       "      <td>7.1531</td>\n",
       "      <td>40.4774</td>\n",
       "      <td>33.3243</td>\n",
       "      <td>5.0341</td>\n",
       "      <td>3.3898</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>2.7003</td>\n",
       "      <td>3.8032</td>\n",
       "      <td>1.1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aparc.DKTatlas</td>\n",
       "      <td>lh</td>\n",
       "      <td>cuneus</td>\n",
       "      <td>4102</td>\n",
       "      <td>2619</td>\n",
       "      <td>5753</td>\n",
       "      <td>2.019</td>\n",
       "      <td>0.490</td>\n",
       "      <td>...</td>\n",
       "      <td>5.2623</td>\n",
       "      <td>-13.1617</td>\n",
       "      <td>33.8137</td>\n",
       "      <td>46.9754</td>\n",
       "      <td>3.0343</td>\n",
       "      <td>3.2453</td>\n",
       "      <td>0.3093</td>\n",
       "      <td>2.4099</td>\n",
       "      <td>3.5491</td>\n",
       "      <td>1.1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aparc.DKTatlas</td>\n",
       "      <td>lh</td>\n",
       "      <td>entorhinal</td>\n",
       "      <td>737</td>\n",
       "      <td>549</td>\n",
       "      <td>2714</td>\n",
       "      <td>3.655</td>\n",
       "      <td>0.585</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0438</td>\n",
       "      <td>2.5989</td>\n",
       "      <td>37.5099</td>\n",
       "      <td>34.9110</td>\n",
       "      <td>3.4560</td>\n",
       "      <td>2.6710</td>\n",
       "      <td>0.1285</td>\n",
       "      <td>2.4654</td>\n",
       "      <td>2.9647</td>\n",
       "      <td>0.4993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aparc.DKTatlas</td>\n",
       "      <td>lh</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>4115</td>\n",
       "      <td>2822</td>\n",
       "      <td>8180</td>\n",
       "      <td>2.738</td>\n",
       "      <td>0.526</td>\n",
       "      <td>...</td>\n",
       "      <td>5.2854</td>\n",
       "      <td>-5.9378</td>\n",
       "      <td>39.6908</td>\n",
       "      <td>45.6286</td>\n",
       "      <td>3.9405</td>\n",
       "      <td>2.8272</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>2.3304</td>\n",
       "      <td>3.1105</td>\n",
       "      <td>0.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeo2011_7Networks_N1000</td>\n",
       "      <td>rh</td>\n",
       "      <td>7Networks_3</td>\n",
       "      <td>14937</td>\n",
       "      <td>9936</td>\n",
       "      <td>27688</td>\n",
       "      <td>2.611</td>\n",
       "      <td>0.492</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0774</td>\n",
       "      <td>-10.8846</td>\n",
       "      <td>39.2314</td>\n",
       "      <td>50.1161</td>\n",
       "      <td>4.1769</td>\n",
       "      <td>3.1173</td>\n",
       "      <td>0.3747</td>\n",
       "      <td>2.4544</td>\n",
       "      <td>4.7044</td>\n",
       "      <td>2.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeo2011_7Networks_N1000</td>\n",
       "      <td>rh</td>\n",
       "      <td>7Networks_4</td>\n",
       "      <td>13382</td>\n",
       "      <td>9146</td>\n",
       "      <td>29555</td>\n",
       "      <td>2.909</td>\n",
       "      <td>0.582</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8317</td>\n",
       "      <td>-41.1954</td>\n",
       "      <td>52.2013</td>\n",
       "      <td>93.3967</td>\n",
       "      <td>3.8157</td>\n",
       "      <td>3.5262</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>1.8828</td>\n",
       "      <td>5.1531</td>\n",
       "      <td>3.2703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13737</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeo2011_7Networks_N1000</td>\n",
       "      <td>rh</td>\n",
       "      <td>7Networks_5</td>\n",
       "      <td>10558</td>\n",
       "      <td>7677</td>\n",
       "      <td>31072</td>\n",
       "      <td>3.196</td>\n",
       "      <td>0.792</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1063</td>\n",
       "      <td>-22.2837</td>\n",
       "      <td>88.8118</td>\n",
       "      <td>111.0955</td>\n",
       "      <td>3.3020</td>\n",
       "      <td>2.5300</td>\n",
       "      <td>0.3971</td>\n",
       "      <td>2.0215</td>\n",
       "      <td>4.7753</td>\n",
       "      <td>2.7538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeo2011_7Networks_N1000</td>\n",
       "      <td>rh</td>\n",
       "      <td>7Networks_6</td>\n",
       "      <td>20144</td>\n",
       "      <td>13602</td>\n",
       "      <td>41999</td>\n",
       "      <td>2.696</td>\n",
       "      <td>0.641</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0781</td>\n",
       "      <td>-11.6287</td>\n",
       "      <td>43.5814</td>\n",
       "      <td>55.2101</td>\n",
       "      <td>3.6592</td>\n",
       "      <td>3.0563</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>1.8599</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>3.0550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13739</th>\n",
       "      <td>sub-1000393599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeo2011_7Networks_N1000</td>\n",
       "      <td>rh</td>\n",
       "      <td>7Networks_7</td>\n",
       "      <td>24278</td>\n",
       "      <td>16614</td>\n",
       "      <td>55069</td>\n",
       "      <td>2.878</td>\n",
       "      <td>0.571</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1819</td>\n",
       "      <td>-18.0298</td>\n",
       "      <td>48.6628</td>\n",
       "      <td>66.6926</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>3.0117</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>1.9099</td>\n",
       "      <td>4.6689</td>\n",
       "      <td>2.7590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13740 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject_id  session_id                    atlas hemisphere  \\\n",
       "0      sub-1000393599         NaN           aparc.DKTatlas         lh   \n",
       "1      sub-1000393599         NaN           aparc.DKTatlas         lh   \n",
       "2      sub-1000393599         NaN           aparc.DKTatlas         lh   \n",
       "3      sub-1000393599         NaN           aparc.DKTatlas         lh   \n",
       "4      sub-1000393599         NaN           aparc.DKTatlas         lh   \n",
       "...               ...         ...                      ...        ...   \n",
       "13735  sub-1000393599         NaN  Yeo2011_7Networks_N1000         rh   \n",
       "13736  sub-1000393599         NaN  Yeo2011_7Networks_N1000         rh   \n",
       "13737  sub-1000393599         NaN  Yeo2011_7Networks_N1000         rh   \n",
       "13738  sub-1000393599         NaN  Yeo2011_7Networks_N1000         rh   \n",
       "13739  sub-1000393599         NaN  Yeo2011_7Networks_N1000         rh   \n",
       "\n",
       "                    StructName  NumVert  SurfArea  GrayVol  ThickAvg  \\\n",
       "0      caudalanteriorcingulate     1668      1121     3493     2.870   \n",
       "1          caudalmiddlefrontal     3308      2236     7030     2.882   \n",
       "2                       cuneus     4102      2619     5753     2.019   \n",
       "3                   entorhinal      737       549     2714     3.655   \n",
       "4                     fusiform     4115      2822     8180     2.738   \n",
       "...                        ...      ...       ...      ...       ...   \n",
       "13735              7Networks_3    14937      9936    27688     2.611   \n",
       "13736              7Networks_4    13382      9146    29555     2.909   \n",
       "13737              7Networks_5    10558      7677    31072     3.196   \n",
       "13738              7Networks_6    20144     13602    41999     2.696   \n",
       "13739              7Networks_7    24278     16614    55069     2.878   \n",
       "\n",
       "       ThickStd  ...  StdDev_wgpct  Min_wgpct  Max_wgpct  Range_wgpct  \\\n",
       "0         0.588  ...        5.8371    -1.8413    42.8855      44.7269   \n",
       "1         0.537  ...        4.6666     7.1531    40.4774      33.3243   \n",
       "2         0.490  ...        5.2623   -13.1617    33.8137      46.9754   \n",
       "3         0.585  ...        6.0438     2.5989    37.5099      34.9110   \n",
       "4         0.526  ...        5.2854    -5.9378    39.6908      45.6286   \n",
       "...         ...  ...           ...        ...        ...          ...   \n",
       "13735     0.492  ...        5.0774   -10.8846    39.2314      50.1161   \n",
       "13736     0.582  ...        5.8317   -41.1954    52.2013      93.3967   \n",
       "13737     0.792  ...        7.1063   -22.2837    88.8118     111.0955   \n",
       "13738     0.641  ...        6.0781   -11.6287    43.5814      55.2101   \n",
       "13739     0.571  ...        6.1819   -18.0298    48.6628      66.6926   \n",
       "\n",
       "       SNR_wgpct  Mean_piallgi  StdDev_piallgi  Min_piallgi  Max_piallgi  \\\n",
       "0         4.4281        1.9877          0.0777       1.8054       2.1455   \n",
       "1         5.0341        3.3898          0.2448       2.7003       3.8032   \n",
       "2         3.0343        3.2453          0.3093       2.4099       3.5491   \n",
       "3         3.4560        2.6710          0.1285       2.4654       2.9647   \n",
       "4         3.9405        2.8272          0.1093       2.3304       3.1105   \n",
       "...          ...           ...             ...          ...          ...   \n",
       "13735     4.1769        3.1173          0.3747       2.4544       4.7044   \n",
       "13736     3.8157        3.5262          0.9928       1.8828       5.1531   \n",
       "13737     3.3020        2.5300          0.3971       2.0215       4.7753   \n",
       "13738     3.6592        3.0563          0.5547       1.8599       4.9149   \n",
       "13739     3.7574        3.0117          0.6360       1.9099       4.6689   \n",
       "\n",
       "       Range_piallgi  \n",
       "0             0.3402  \n",
       "1             1.1029  \n",
       "2             1.1392  \n",
       "3             0.4993  \n",
       "4             0.7800  \n",
       "...              ...  \n",
       "13735         2.2500  \n",
       "13736         3.2703  \n",
       "13737         2.7538  \n",
       "13738         3.0550  \n",
       "13739         2.7590  \n",
       "\n",
       "[13740 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can construct new paths by using the `/` operator. This is identical to\n",
    "# how paths are constructed in the `pathlib` module.\n",
    "stats_filepath = sub_path / f'sub-{subject_id}_regionsurfacestats.tsv'\n",
    "\n",
    "# Use pandas to read in the TSV file then display it:\n",
    "\n",
    "print(f\"Loading {stats_filepath} ...\")\n",
    "with stats_filepath.open('r') as f:\n",
    "    data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac0df6-9208-4d85-8a70-044f60ae2f6c",
   "metadata": {},
   "source": [
    "### Getting the Participant Lists and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b349df-15c8-42a0-a18b-1a7e6a6b28c2",
   "metadata": {},
   "source": [
    "We have pre-sorted the participants in the PNC study into a training and a\n",
    "test dataset. Basic metadata about each participant can be found in TSV files\n",
    "in the `shared` directory in your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1032c95-1ca5-4ef5-96d1-8f51acf260d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>study</th>\n",
       "      <th>study_site</th>\n",
       "      <th>session_id</th>\n",
       "      <th>wave</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>bmi</th>\n",
       "      <th>handedness</th>\n",
       "      <th>participant_education</th>\n",
       "      <th>parent_1_education</th>\n",
       "      <th>parent_2_education</th>\n",
       "      <th>p_factor</th>\n",
       "      <th>internalizing_mcelroy_harmonized_all_samples</th>\n",
       "      <th>externalizing_mcelroy_harmonized_all_samples</th>\n",
       "      <th>attention_mcelroy_harmonized_all_samples</th>\n",
       "      <th>cubids_acquisition_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000393599</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.583333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>22.15</td>\n",
       "      <td>Right</td>\n",
       "      <td>9th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>0.589907</td>\n",
       "      <td>-0.449373</td>\n",
       "      <td>-0.630780</td>\n",
       "      <td>-1.842178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001970838</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.833333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Other</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>23.98</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>-0.659061</td>\n",
       "      <td>0.531072</td>\n",
       "      <td>0.392751</td>\n",
       "      <td>0.190706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1007995238</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>23.77</td>\n",
       "      <td>Right</td>\n",
       "      <td>6th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-1.608375</td>\n",
       "      <td>-0.744118</td>\n",
       "      <td>-0.314187</td>\n",
       "      <td>-0.432662</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1011497669</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>29.68</td>\n",
       "      <td>Right</td>\n",
       "      <td>9th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>-1.233807</td>\n",
       "      <td>-0.896835</td>\n",
       "      <td>-0.449099</td>\n",
       "      <td>0.111167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017092387</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>23.24</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-0.923100</td>\n",
       "      <td>-0.313455</td>\n",
       "      <td>2.204168</td>\n",
       "      <td>-0.782266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>969649154</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>17.38</td>\n",
       "      <td>Right</td>\n",
       "      <td>5th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.148520</td>\n",
       "      <td>0.556444</td>\n",
       "      <td>0.024228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>970890500</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>30.89</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993806</td>\n",
       "      <td>1.578177</td>\n",
       "      <td>-0.373470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>975856179</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>15.67</td>\n",
       "      <td>Right</td>\n",
       "      <td>4th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.026645</td>\n",
       "      <td>-0.582212</td>\n",
       "      <td>1.333857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>984757368</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.416667</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>16.66</td>\n",
       "      <td>Right</td>\n",
       "      <td>5th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.360029</td>\n",
       "      <td>-0.515655</td>\n",
       "      <td>1.509584</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>987544292</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.416667</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.399735</td>\n",
       "      <td>-0.490472</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1601 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant_id study study_site session_id  wave        age     sex  \\\n",
       "0        1000393599   PNC       PNC1       PNC1     1  15.583333    Male   \n",
       "1        1001970838   PNC       PNC1       PNC1     1  17.833333    Male   \n",
       "2        1007995238   PNC       PNC1       PNC1     1  13.750000  Female   \n",
       "3        1011497669   PNC       PNC1       PNC1     1  16.666667    Male   \n",
       "4        1017092387   PNC       PNC1       PNC1     1  18.666667  Female   \n",
       "..              ...   ...        ...        ...   ...        ...     ...   \n",
       "529       969649154   PNC       PNC1       PNC1     1  12.333333    Male   \n",
       "530       970890500   PNC       PNC1       PNC1     1  18.166667  Female   \n",
       "531       975856179   PNC       PNC1       PNC1     1  11.000000    Male   \n",
       "532       984757368   PNC       PNC1       PNC1     1  13.416667    Male   \n",
       "533       987544292   PNC       PNC1       PNC1     1  18.416667  Female   \n",
       "\n",
       "      race               ethnicity    bmi handedness participant_education  \\\n",
       "0    Black  not Hispanic or Latino  22.15      Right             9th Grade   \n",
       "1    Other      Hispanic or Latino  23.98      Right            11th Grade   \n",
       "2    Other  not Hispanic or Latino  23.77      Right             6th Grade   \n",
       "3    White  not Hispanic or Latino  29.68      Right             9th Grade   \n",
       "4    Black  not Hispanic or Latino  23.24      Right            11th Grade   \n",
       "..     ...                     ...    ...        ...                   ...   \n",
       "529  White  not Hispanic or Latino  17.38      Right             5th Grade   \n",
       "530  White  not Hispanic or Latino  30.89      Right            11th Grade   \n",
       "531  White  not Hispanic or Latino  15.67      Right             4th Grade   \n",
       "532  Black  not Hispanic or Latino  16.66      Right             5th Grade   \n",
       "533  White  not Hispanic or Latino    NaN      Right            11th Grade   \n",
       "\n",
       "     parent_1_education  parent_2_education  p_factor  \\\n",
       "0      Complete primary  Complete secondary  0.589907   \n",
       "1     Complete tertiary   Complete tertiary -0.659061   \n",
       "2     Complete tertiary    Complete primary -1.608375   \n",
       "3     Complete tertiary   Complete tertiary -1.233807   \n",
       "4      Complete primary    Complete primary -0.923100   \n",
       "..                  ...                 ...       ...   \n",
       "529   Complete tertiary  Complete secondary       NaN   \n",
       "530  Complete secondary  Complete secondary       NaN   \n",
       "531    Complete primary  Complete secondary       NaN   \n",
       "532    Complete primary                 NaN       NaN   \n",
       "533  Complete secondary    Complete primary       NaN   \n",
       "\n",
       "     internalizing_mcelroy_harmonized_all_samples  \\\n",
       "0                                       -0.449373   \n",
       "1                                        0.531072   \n",
       "2                                       -0.744118   \n",
       "3                                       -0.896835   \n",
       "4                                       -0.313455   \n",
       "..                                            ...   \n",
       "529                                     -0.148520   \n",
       "530                                      0.993806   \n",
       "531                                     -1.026645   \n",
       "532                                      0.360029   \n",
       "533                                      0.399735   \n",
       "\n",
       "     externalizing_mcelroy_harmonized_all_samples  \\\n",
       "0                                       -0.630780   \n",
       "1                                        0.392751   \n",
       "2                                       -0.314187   \n",
       "3                                       -0.449099   \n",
       "4                                        2.204168   \n",
       "..                                            ...   \n",
       "529                                      0.556444   \n",
       "530                                      1.578177   \n",
       "531                                     -0.582212   \n",
       "532                                     -0.515655   \n",
       "533                                     -0.490472   \n",
       "\n",
       "     attention_mcelroy_harmonized_all_samples  cubids_acquisition_group  \n",
       "0                                   -1.842178                         1  \n",
       "1                                    0.190706                         1  \n",
       "2                                   -0.432662                         1  \n",
       "3                                    0.111167                         1  \n",
       "4                                   -0.782266                         1  \n",
       "..                                        ...                       ...  \n",
       "529                                  0.024228                         1  \n",
       "530                                 -0.373470                         1  \n",
       "531                                  1.333857                         1  \n",
       "532                                  1.509584                       114  \n",
       "533                                  0.018679                         1  \n",
       "\n",
       "[1601 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Participant meta-data is generally located in the BIDS repository for each\n",
    "# study:\n",
    "rbcdata_path = Path('/home/jovyan/shared/data/RBC')\n",
    "train_filepath = rbcdata_path / 'train_participants.tsv'\n",
    "test_filepath = rbcdata_path / 'test_participants.tsv'\n",
    "\n",
    "# Load the PNC participants TSV files...\n",
    "with train_filepath.open('r') as f:\n",
    "    train_data = pd.read_csv(f, sep='\\t')\n",
    "with test_filepath.open('r') as f:\n",
    "    test_data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# We can also concatenate the two datasets into a single dataset of all\n",
    "# study participants:\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "\n",
    "# Display the full dataframe:\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe91dd5-795a-4368-b171-fe070d456884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>study</th>\n",
       "      <th>study_site</th>\n",
       "      <th>session_id</th>\n",
       "      <th>wave</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>bmi</th>\n",
       "      <th>handedness</th>\n",
       "      <th>participant_education</th>\n",
       "      <th>parent_1_education</th>\n",
       "      <th>parent_2_education</th>\n",
       "      <th>p_factor</th>\n",
       "      <th>internalizing_mcelroy_harmonized_all_samples</th>\n",
       "      <th>externalizing_mcelroy_harmonized_all_samples</th>\n",
       "      <th>attention_mcelroy_harmonized_all_samples</th>\n",
       "      <th>cubids_acquisition_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000393599</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.583333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>22.15</td>\n",
       "      <td>Right</td>\n",
       "      <td>9th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>0.589907</td>\n",
       "      <td>-0.449373</td>\n",
       "      <td>-0.630780</td>\n",
       "      <td>-1.842178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001970838</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.833333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Other</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>23.98</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>-0.659061</td>\n",
       "      <td>0.531072</td>\n",
       "      <td>0.392751</td>\n",
       "      <td>0.190706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1007995238</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>23.77</td>\n",
       "      <td>Right</td>\n",
       "      <td>6th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-1.608375</td>\n",
       "      <td>-0.744118</td>\n",
       "      <td>-0.314187</td>\n",
       "      <td>-0.432662</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1011497669</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>29.68</td>\n",
       "      <td>Right</td>\n",
       "      <td>9th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>-1.233807</td>\n",
       "      <td>-0.896835</td>\n",
       "      <td>-0.449099</td>\n",
       "      <td>0.111167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017092387</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>23.24</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-0.923100</td>\n",
       "      <td>-0.313455</td>\n",
       "      <td>2.204168</td>\n",
       "      <td>-0.782266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>983504031</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.083333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>22.81</td>\n",
       "      <td>Left</td>\n",
       "      <td>9th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-1.262053</td>\n",
       "      <td>1.508464</td>\n",
       "      <td>-0.436507</td>\n",
       "      <td>-0.599140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>985910486</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>24.50</td>\n",
       "      <td>Right</td>\n",
       "      <td>12th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.233807</td>\n",
       "      <td>-0.896835</td>\n",
       "      <td>-0.449099</td>\n",
       "      <td>0.111167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>986035435</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.916667</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>3rd Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-0.872749</td>\n",
       "      <td>1.581768</td>\n",
       "      <td>-0.619987</td>\n",
       "      <td>0.556958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>993394555</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-1.420477</td>\n",
       "      <td>0.750985</td>\n",
       "      <td>-0.377146</td>\n",
       "      <td>-0.519601</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>997818717</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.916667</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>1.088511</td>\n",
       "      <td>1.007551</td>\n",
       "      <td>-0.494069</td>\n",
       "      <td>-2.132590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1067 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      participant_id study study_site session_id  wave        age     sex  \\\n",
       "0         1000393599   PNC       PNC1       PNC1     1  15.583333    Male   \n",
       "1         1001970838   PNC       PNC1       PNC1     1  17.833333    Male   \n",
       "2         1007995238   PNC       PNC1       PNC1     1  13.750000  Female   \n",
       "3         1011497669   PNC       PNC1       PNC1     1  16.666667    Male   \n",
       "4         1017092387   PNC       PNC1       PNC1     1  18.666667  Female   \n",
       "...              ...   ...        ...        ...   ...        ...     ...   \n",
       "1062       983504031   PNC       PNC1       PNC1     1  16.083333    Male   \n",
       "1063       985910486   PNC       PNC1       PNC1     1  18.750000  Female   \n",
       "1064       986035435   PNC       PNC1       PNC1     1   9.916667  Female   \n",
       "1065       993394555   PNC       PNC1       PNC1     1  19.500000  Female   \n",
       "1066       997818717   PNC       PNC1       PNC1     1  20.916667  Female   \n",
       "\n",
       "       race               ethnicity    bmi handedness participant_education  \\\n",
       "0     Black  not Hispanic or Latino  22.15      Right             9th Grade   \n",
       "1     Other      Hispanic or Latino  23.98      Right            11th Grade   \n",
       "2     Other  not Hispanic or Latino  23.77      Right             6th Grade   \n",
       "3     White  not Hispanic or Latino  29.68      Right             9th Grade   \n",
       "4     Black  not Hispanic or Latino  23.24      Right            11th Grade   \n",
       "...     ...                     ...    ...        ...                   ...   \n",
       "1062  Black  not Hispanic or Latino  22.81       Left             9th Grade   \n",
       "1063  Black  not Hispanic or Latino  24.50      Right            12th Grade   \n",
       "1064  White  not Hispanic or Latino    NaN      Right             3rd Grade   \n",
       "1065  White  not Hispanic or Latino    NaN      Right          Some College   \n",
       "1066  White  not Hispanic or Latino    NaN      Right          Some College   \n",
       "\n",
       "      parent_1_education  parent_2_education  p_factor  \\\n",
       "0       Complete primary  Complete secondary  0.589907   \n",
       "1      Complete tertiary   Complete tertiary -0.659061   \n",
       "2      Complete tertiary    Complete primary -1.608375   \n",
       "3      Complete tertiary   Complete tertiary -1.233807   \n",
       "4       Complete primary    Complete primary -0.923100   \n",
       "...                  ...                 ...       ...   \n",
       "1062    Complete primary    Complete primary -1.262053   \n",
       "1063    Complete primary                 NaN -1.233807   \n",
       "1064    Complete primary    Complete primary -0.872749   \n",
       "1065  Complete secondary  Complete secondary -1.420477   \n",
       "1066  Complete secondary    Complete primary  1.088511   \n",
       "\n",
       "      internalizing_mcelroy_harmonized_all_samples  \\\n",
       "0                                        -0.449373   \n",
       "1                                         0.531072   \n",
       "2                                        -0.744118   \n",
       "3                                        -0.896835   \n",
       "4                                        -0.313455   \n",
       "...                                            ...   \n",
       "1062                                      1.508464   \n",
       "1063                                     -0.896835   \n",
       "1064                                      1.581768   \n",
       "1065                                      0.750985   \n",
       "1066                                      1.007551   \n",
       "\n",
       "      externalizing_mcelroy_harmonized_all_samples  \\\n",
       "0                                        -0.630780   \n",
       "1                                         0.392751   \n",
       "2                                        -0.314187   \n",
       "3                                        -0.449099   \n",
       "4                                         2.204168   \n",
       "...                                            ...   \n",
       "1062                                     -0.436507   \n",
       "1063                                     -0.449099   \n",
       "1064                                     -0.619987   \n",
       "1065                                     -0.377146   \n",
       "1066                                     -0.494069   \n",
       "\n",
       "      attention_mcelroy_harmonized_all_samples  cubids_acquisition_group  \n",
       "0                                    -1.842178                         1  \n",
       "1                                     0.190706                         1  \n",
       "2                                    -0.432662                         1  \n",
       "3                                     0.111167                         1  \n",
       "4                                    -0.782266                         1  \n",
       "...                                        ...                       ...  \n",
       "1062                                 -0.599140                         1  \n",
       "1063                                  0.111167                         1  \n",
       "1064                                  0.556958                         1  \n",
       "1065                                 -0.519601                         7  \n",
       "1066                                 -2.132590                         1  \n",
       "\n",
       "[1067 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd455303-7695-4aff-b2fa-b4b3435f8e7c",
   "metadata": {},
   "source": [
    "## Project Goal: Predict the `p_factor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbcabd-b561-4126-a21d-e535ae259ae6",
   "metadata": {},
   "source": [
    "The RBC datasets include a variable for each subject called the `p_factor`.\n",
    "This factor is intended to capture overall psychopathology and is discussed at\n",
    "length in RBC publications. The goal for this project is to train a\n",
    "machine-learning tool to predict the `p_factor` of each participant in the\n",
    "test dataset by using data from the participants in the training dataset.\n",
    "Note that the `p_factor` column in the training dataset is provided, but the\n",
    "`p_factor` column in the test dataset has been set to `NaN`.\n",
    "\n",
    "Your specific task is to calculate predicted `p_factor` values, to insert\n",
    "these values into the `'p_factor'` column of the provided `test_data`\n",
    "dataframe, to save `test_data` to disk using the `test_data.to_csv` method\n",
    "(example below), then finally to commit and push the file to your group's\n",
    "GitHub repository.\n",
    "\n",
    "We will look over the results of the group mini-projects together once\n",
    "everyone has submitted their predictions.\n",
    "\n",
    "**In this section, we demonstrate an example approach to predicting the\n",
    "`p_factor` using one of the most straightforward supervised techniques in\n",
    "machine learning: linear regression.** Suppose we suspected that the size of\n",
    "Brodmann Area 1 was predictive of the `p_factor` in individual participants\n",
    "and thus wanted to run a linear regression analysis to predict `p_factor` in\n",
    "the test participants based on the relationship in the training participants.\n",
    "Performing linear regression will require a few steps, which are likely to be\n",
    "similar in your projects:\n",
    "\n",
    "1. Collect the relevant data (the surface areas of BA1) into a dataframe.\n",
    "2. Train the linear regression model using the training participants.\n",
    "3. Use the trained model to predict the `p_factor` of the test subjects.\n",
    "4. Export and commit our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929ed1a-02a9-424a-b05c-6f97e71bf052",
   "metadata": {},
   "source": [
    "### Step 1. Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a5bad-e052-44c2-8064-ebf76a121284",
   "metadata": {},
   "source": [
    "The data we need to make the predictions are, for each participant, (1) the\n",
    "surface area of BA1, and (2) the `p_factor`. We can collect these into a\n",
    "dataframe using `pandas` and the `RBCPath` type (to load the data).\n",
    "\n",
    "The surface area of BA1 can be found in the FreeSurfer TSV files examined\n",
    "earlier in this notebook. We'll start by writing a function that loads the\n",
    "appropriate TSV for for a given participant.\n",
    "\n",
    "In order to speed up the loading of data during the project, we can specify\n",
    "a `local_cache_dir` where any data downloaded using the function will be\n",
    "automatically saved; the next time you load the same data, it will be loaded\n",
    "from local storage instead of from S3 (local storage is much faster). The\n",
    "function here uses the directory `cache` in your home directory by default,\n",
    "but you can change this if you prefer a different directory. The directory\n",
    "will be automatically created for you if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74b7d4-132f-4ce8-bc02-2d3a6fa1b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fsdata(participant_id, local_cache_dir=(Path.home() / 'cache')):\n",
    "    \"Loads and returns the dataframe of a PNC participant's FreeSurfer data.\"\n",
    "\n",
    "    # Check that the local_cache_dir exists and make it if it doesn't.\n",
    "    if local_cache_dir is not None:\n",
    "        local_cache_dir = Path(local_cache_dir)\n",
    "        local_cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Make the RBCPath and find the appropriate file:\n",
    "    pnc_freesurfer_path = RBCPath(\n",
    "        'rbc://PNC_FreeSurfer/freesurfer',\n",
    "        # We provide the local_cache_dir to the RBCPath object; all paths made\n",
    "        # from this object will use the same cache directory.\n",
    "        local_cache_dir=local_cache_dir)\n",
    "    participant_path = pnc_freesurfer_path / f'sub-{participant_id}'\n",
    "    tsv_path = participant_path / f'sub-{participant_id}_regionsurfacestats.tsv'\n",
    "\n",
    "    # Use pandas to read in the TSV file:\n",
    "    with tsv_path.open('r') as f:\n",
    "        data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "    # Return the loaded data:\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7ecff-4fa0-47b8-9be4-b133d2f8c4e0",
   "metadata": {},
   "source": [
    "We can run this function to obtain a subject's FreeSurfer dataframe. This\n",
    "dataframe contains information about various anatomical atlases that segment\n",
    "the cortical surface into distinct regions. Notice that the `atlas` column of\n",
    "the dataframe contains the name of distinct atlases while the `StructName`\n",
    "column contains the name of the ROI described. The `SurfArea` column gives the\n",
    "surface area of each ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a479fe4-e051-442d-a780-c036d3ff91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_participant_id = 1000393599\n",
    "data = load_fsdata(example_participant_id)\n",
    "\n",
    "# Display the dataframe we loaded:\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93998d-0b3a-4aad-ba48-8640e7634f5c",
   "metadata": {},
   "source": [
    "To extract the surface area of BA1, we need to look for rows whose\n",
    "`StructName` indicates that it represents BA1. In the RBC database, the name\n",
    "`'Brodmann.1'` is used to represent Brodmann Area 1. We can select only the\n",
    "rows of interest using this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71c78d-1d54-420c-b908-a0f352c4e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_mask = (data['StructName'] == 'Brodmann.1')\n",
    "data[row_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c02b4a-7515-404a-86a2-c7c2286e727e",
   "metadata": {},
   "source": [
    "Given these rows, we can extract the BA1 surface areas and sum them (we will\n",
    "perform the linear regression on the bilateral BA1 surface area by adding the\n",
    "left and right hemisphere surface areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be77dd-3a54-4f46-8d08-ced65589622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba1_surfareas = data.loc[row_mask, 'SurfArea']\n",
    "ba1_surfarea = sum(ba1_surfareas)\n",
    "\n",
    "# Show the bilateral surface area for this participant (in square mm):\n",
    "ba1_surfarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af5c9a-19e5-499d-9ced-792a6e0010da",
   "metadata": {},
   "source": [
    "Based on the above workflow, we can now write a function that extracts the BA1\n",
    "surface area for a participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27acf24-98b7-4388-b5a5-13ecee644ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ba1_surfarea(participant_id):\n",
    "    \"\"\"Loads and returns the bilateral Brodmann Area 1 surface area for a PNC\n",
    "    study participant.\n",
    "    \"\"\"\n",
    "    # First, load the subject's FreeSurfer dataframe:\n",
    "    data = load_fsdata(participant_id)\n",
    "    # Next, find the relevant rows:\n",
    "    row_mask = (data['StructName'] == 'Brodmann.1')\n",
    "    # Then extract and sum the surface areas:\n",
    "    ba1_surfareas = data.loc[row_mask, 'SurfArea']\n",
    "    ba1_surfarea = sum(ba1_surfareas)\n",
    "    # And return this value:\n",
    "    return ba1_surfarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae057b10-19ad-46ec-b276-0d2ea4d1c766",
   "metadata": {},
   "source": [
    "Let's test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63231c34-aad6-4c89-ba69-e79d60ed2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ba1_surfarea(example_participant_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadf9f4-d35b-4d5b-ab51-34819905f396",
   "metadata": {},
   "source": [
    "Now that we have code written to extract data from a single subject, we can\n",
    "gather the BA1 data for our training and test subjects into a single\n",
    "dataframe. Doing so will require downloading all of the TSV files for all of\n",
    "the subjects in the training dataset. This will take some time, but probably\n",
    "less than an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d268e-d0e8-48b4-b1ba-3006ba536d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load in surface area data for each participant:\n",
    "print(\"Loading surface areas...\")     \n",
    "\n",
    "# We will put the rows in this dictionary of lists as we build the dataframe:\n",
    "all_vars = {\n",
    "    'participant_id': [],\n",
    "    'ba1_surface_area': [],\n",
    "    'p_factor': []}\n",
    "\n",
    "# We'll display a progress bar `prog` as we go also:\n",
    "from ipywidgets import IntProgress\n",
    "prog = IntProgress(min=0, max=len(all_data))\n",
    "display(prog)\n",
    "\n",
    "# Okay, loop through each row of the `all_data` dataframe, which contains both\n",
    "# training and test subjects, load their BA1 data, and store it in the\n",
    "# all_vars dictionary.\n",
    "for (ii, row) in all_data.iterrows():\n",
    "    # Extract the participant ID and p_factor (which will be NaN for test\n",
    "    # participants).\n",
    "    participant_id = row['participant_id']\n",
    "    p_factor = row['p_factor']\n",
    "    # Load the surface area for this participant:\n",
    "    try:\n",
    "        surf_area = load_ba1_surfarea(participant_id)\n",
    "    except FileNotFoundError:\n",
    "        # Some subjects are just missing the file, so we code them as NaN.\n",
    "        surf_area = np.nan\n",
    "    # Append the participant ID and their surface area to our dataset:\n",
    "    all_vars['participant_id'].append(participant_id)\n",
    "    all_vars['ba1_surface_area'].append(surf_area)\n",
    "    all_vars['p_factor'].append(p_factor)\n",
    "    # Increment the progress bar counter:\n",
    "    prog.value += 1\n",
    "\n",
    "# Convert train_vars into a dataframe.\n",
    "all_vars = pd.DataFrame(all_vars)\n",
    "\n",
    "# Extract the training and test subjects into separate dataframes; the test\n",
    "# participants can be identified as those having NaN values for their\n",
    "# p_factor column.\n",
    "train_vars = all_vars[~np.isnan(all_vars['p_factor'])]\n",
    "test_vars = all_vars[np.isnan(all_vars['p_factor'])]\n",
    "\n",
    "# Display the finished dataframe.\n",
    "all_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80cd37-6807-45cb-81ae-6f0369a0540c",
   "metadata": {},
   "source": [
    "### Step 2. Train the Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff330c49-67db-45bc-afc2-63ff895424a4",
   "metadata": {},
   "source": [
    "To train and perform the linear regression analysis, we will use the\n",
    "[`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "type from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a65c2-03ae-44f8-8ddf-58989aa9c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression type:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# LinearRegression requires a matrix whose columns are the variables and whose\n",
    "# final column is the value being predicted (the p_factor for us). We can\n",
    "# extract these columns straight from the dataframes we generated.\n",
    "train_matrix = train_vars.loc[:, ['ba1_surface_area', 'p_factor']].values\n",
    "# We need to exclude rows with NaNs for training:\n",
    "train_okrows = np.all(~np.isnan(train_matrix), axis=1)\n",
    "train_matrix = train_matrix[train_okrows]\n",
    "\n",
    "# Train the regression using the training matrix:\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(train_matrix[:, :1], train_matrix[:, 1])\n",
    "\n",
    "# Display the trained regression parameters:\n",
    "print(\"Linear Regression:\")\n",
    "print(\"  Intercept:\", lreg.intercept_)\n",
    "print(\"  Slope:\", lreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766245cf-94c9-42b0-a42b-2ed7190b2e80",
   "metadata": {},
   "source": [
    "### Step 3. Predict the `p_factor` of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06a34b-c206-4c8f-bc3a-fe7afee79edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply the trained linear regression object `lreg` to the 1-column\n",
    "# matrix of ba1_surface_area values in the test_vars dataframe.\n",
    "test_matrix = test_vars.loc[:, ['ba1_surface_area']].values\n",
    "test_okrows = np.all(~np.isnan(test_matrix), axis=1)\n",
    "test_matrix = test_matrix[test_okrows]\n",
    "\n",
    "# Apply the model:\n",
    "p_factor_predictions = lreg.predict(test_matrix)\n",
    "\n",
    "# Display the predictions:\n",
    "p_factor_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9dc36-0778-4fd5-9a92-598922c998b4",
   "metadata": {},
   "source": [
    "### Step 4. Save and Commit the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23498-57b9-4e6d-a103-4e1f4afdb177",
   "metadata": {},
   "source": [
    "To save and commit the results, we first need to save the predicted `p_factor`\n",
    "data into the test dataframe (where there are currently NaNs). In the cell\n",
    "above, we calculated the variable `test_okrows` that indicates which rows\n",
    "of the `test_vars`, `test_matrix`, and `test_data` objects were predicted\n",
    "(those that weren't predicted were excluded due to missing surface area data\n",
    "in our case).\n",
    "\n",
    "We can use this to insert the predicted `p_factor` data into `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d718f88-14b8-4b90-bf73-25e53dafc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[test_okrows, 'p_factor'] = p_factor_predictions\n",
    "\n",
    "# Display the resulting test data:\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d197ec-61ca-4225-ab51-82087d4b48e8",
   "metadata": {},
   "source": [
    "We now need to save the data to disk. We want to put this in the `results`\n",
    "directory of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5bcfe-3ff9-4c78-9782-1777a1ff9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sep='\\t' option here is necessary for tab-separated-value (as opposed to\n",
    "# comma-separated-value) files. The `index=False` just indicates that pandas\n",
    "# doesn't need to write out its own index column.\n",
    "\n",
    "group_name = 'example'  # Change this to be your group name!\n",
    "\n",
    "test_data.to_csv(f'results/{group_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4f18b-72b8-4b46-9ff6-bcdf1b699d35",
   "metadata": {},
   "source": [
    "Once the tsv file has been saved, you can commit it to your GitHub repository\n",
    "then push it and submit a pull request to the `results` branch of the original\n",
    "repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
